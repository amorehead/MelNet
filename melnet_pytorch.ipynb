{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# if importlib.util.find_spec('jamo') is None:\n",
    "#   !pip install jamo\n",
    "# if importlib.util.find_spec('audiosegment') is None:\n",
    "#   !pip install audiosegment\n",
    "# if importlib.util.find_spec('unidecode') is None:\n",
    "#   !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if importlib.util.find_spec('google') is not None:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define imports\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import platform\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from jamo import h2j, j2h\n",
    "from jamo.jamo import _jamo_char_to_hcj\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "from jamo import hangul_to_jamo, h2j, j2h\n",
    "import random\n",
    "import subprocess\n",
    "import audiosegment\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import librosa\n",
    "\n",
    "import yaml\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "from unidecode import unidecode  # Added to support LJ_speech\n",
    "import inflect\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants in \"constant.py\"\n",
    "# x:y = tiers:(axis should be divisible by)\n",
    "t_div = {1:1, 2:1, 3:2, 4:2, 5:4, 6:4}\n",
    "f_div = {1:1, 2:1, 3:2, 4:2, 5:4, 6:4, 7:8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants in \"utils.py\"\n",
    "PAD = '_'\n",
    "EOS = '~'\n",
    "PUNC = '!$%&*\\'(),-.:;?`'\n",
    "SPACE = ' '\n",
    "NUMBERS = '0123456789'\n",
    "SYMBOLS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "en_symbols = SYMBOLS + NUMBERS + PAD + EOS + PUNC + SPACE\n",
    "_symbol_to_id = {s: i for i, s in enumerate(en_symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants from \"korean.py\"\n",
    "PAD = '_'\n",
    "EOS = '~'\n",
    "PUNC = '!\\'(),-.:;?'\n",
    "SPACE = ' '\n",
    "\n",
    "JAMO_LEADS = \"\".join([chr(_) for _ in range(0x1100, 0x1113)])\n",
    "JAMO_VOWELS = \"\".join([chr(_) for _ in range(0x1161, 0x1176)])\n",
    "JAMO_TAILS = \"\".join([chr(_) for _ in range(0x11A8, 0x11C3)])\n",
    "\n",
    "VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + PUNC + SPACE\n",
    "ALL_SYMBOLS = PAD + EOS + VALID_CHARS\n",
    "\n",
    "char_to_id = {c: i for i, c in enumerate(ALL_SYMBOLS)}\n",
    "id_to_char = {i: c for i, c in enumerate(ALL_SYMBOLS)}\n",
    "\n",
    "quote_checker = \"\"\"([`\"'＂“‘])(.+?)([`\"'＂”’])\"\"\"\n",
    "\n",
    "num_to_kor = {\n",
    "        '0': '영',\n",
    "        '1': '일',\n",
    "        '2': '이',\n",
    "        '3': '삼',\n",
    "        '4': '사',\n",
    "        '5': '오',\n",
    "        '6': '육',\n",
    "        '7': '칠',\n",
    "        '8': '팔',\n",
    "        '9': '구',\n",
    "}\n",
    "\n",
    "unit_to_kor1 = {\n",
    "        '%': '퍼센트',\n",
    "        'cm': '센치미터',\n",
    "        'mm': '밀리미터',\n",
    "        'km': '킬로미터',\n",
    "        'kg': '킬로그람',\n",
    "}\n",
    "unit_to_kor2 = {\n",
    "        'm': '미터',\n",
    "}\n",
    "\n",
    "upper_to_kor = {\n",
    "        'A': '에이',\n",
    "        'B': '비',\n",
    "        'C': '씨',\n",
    "        'D': '디',\n",
    "        'E': '이',\n",
    "        'F': '에프',\n",
    "        'G': '지',\n",
    "        'H': '에이치',\n",
    "        'I': '아이',\n",
    "        'J': '제이',\n",
    "        'K': '케이',\n",
    "        'L': '엘',\n",
    "        'M': '엠',\n",
    "        'N': '엔',\n",
    "        'O': '오',\n",
    "        'P': '피',\n",
    "        'Q': '큐',\n",
    "        'R': '알',\n",
    "        'S': '에스',\n",
    "        'T': '티',\n",
    "        'U': '유',\n",
    "        'V': '브이',\n",
    "        'W': '더블유',\n",
    "        'X': '엑스',\n",
    "        'Y': '와이',\n",
    "        'Z': '지',\n",
    "}\n",
    "\n",
    "number_checker = \"([+-]?\\d[\\d,]*)[\\.]?\\d*\"\n",
    "count_checker = \"(시|명|가지|살|마리|포기|송이|수|톨|통|점|개|벌|척|채|다발|그루|자루|줄|켤레|그릇|잔|마디|상자|사람|곡|병|판)\"\n",
    "\n",
    "num_to_kor1 = [\"\"] + list(\"일이삼사오육칠팔구\")\n",
    "num_to_kor2 = [\"\"] + list(\"만억조경해\")\n",
    "num_to_kor3 = [\"\"] + list(\"십백천\")\n",
    "\n",
    "#count_to_kor1 = [\"\"] + [\"하나\",\"둘\",\"셋\",\"넷\",\"다섯\",\"여섯\",\"일곱\",\"여덟\",\"아홉\"]\n",
    "count_to_kor1 = [\"\"] + [\"한\",\"두\",\"세\",\"네\",\"다섯\",\"여섯\",\"일곱\",\"여덟\",\"아홉\"]\n",
    "\n",
    "count_tenth_dict = {\n",
    "        \"십\": \"열\",\n",
    "        \"두십\": \"스물\",\n",
    "        \"세십\": \"서른\",\n",
    "        \"네십\": \"마흔\",\n",
    "        \"다섯십\": \"쉰\",\n",
    "        \"여섯십\": \"예순\",\n",
    "        \"일곱십\": \"일흔\",\n",
    "        \"여덟십\": \"여든\",\n",
    "        \"아홉십\": \"아흔\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbols from \"symbols.py\"\n",
    "# coding: utf-8\n",
    "'''\n",
    "Defines the set of symbols used in text input to the model.\n",
    "\n",
    "The default is a set of ASCII characters that works well for English or text that has been run\n",
    "through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n",
    "'''\n",
    "\n",
    "# For english\n",
    "en_symbols = SYMBOLS + NUMBERS + PAD + EOS + PUNC + SPACE  #<-For deployment(Because korean ALL_SYMBOLS follow this convention)\n",
    "symbols = ALL_SYMBOLS # for korean\n",
    "\n",
    "\"\"\"\n",
    "초성과 종성은 같아보이지만, 다른 character이다.\n",
    "\n",
    "'_~ᄀᄁᄂᄃᄄᄅᄆᄇᄈᄉᄊᄋᄌᄍᄎᄏᄐᄑ하ᅢᅣᅤᅥᅦᅧᅨᅩᅪᅫᅬᅭᅮᅯᅰᅱᅲᅳᅴᅵᆨᆩᆪᆫᆬᆭᆮᆯᆰᆱᆲᆳᆴᆵᆶᆷᆸᆹᆺᆻᆼᆽᆾᆿᇀᇁᇂ!'(),-.:;? '\n",
    "\n",
    "'_': 0, '~': 1, 'ᄀ': 2, 'ᄁ': 3, 'ᄂ': 4, 'ᄃ': 5, 'ᄄ': 6, 'ᄅ': 7, 'ᄆ': 8, 'ᄇ': 9, 'ᄈ': 10, \n",
    "'ᄉ': 11, 'ᄊ': 12, 'ᄋ': 13, 'ᄌ': 14, 'ᄍ': 15, 'ᄎ': 16, 'ᄏ': 17, 'ᄐ': 18, 'ᄑ': 19, 'ᄒ': 20, \n",
    "'ᅡ': 21, 'ᅢ': 22, 'ᅣ': 23, 'ᅤ': 24, 'ᅥ': 25, 'ᅦ': 26, 'ᅧ': 27, 'ᅨ': 28, 'ᅩ': 29, 'ᅪ': 30, \n",
    "'ᅫ': 31, 'ᅬ': 32, 'ᅭ': 33, 'ᅮ': 34, 'ᅯ': 35, 'ᅰ': 36, 'ᅱ': 37, 'ᅲ': 38, 'ᅳ': 39, 'ᅴ': 40, \n",
    "'ᅵ': 41, 'ᆨ': 42, 'ᆩ': 43, 'ᆪ': 44, 'ᆫ': 45, 'ᆬ': 46, 'ᆭ': 47, 'ᆮ': 48, 'ᆯ': 49, 'ᆰ': 50, \n",
    "'ᆱ': 51, 'ᆲ': 52, 'ᆳ': 53, 'ᆴ': 54, 'ᆵ': 55, 'ᆶ': 56, 'ᆷ': 57, 'ᆸ': 58, 'ᆹ': 59, 'ᆺ': 60, \n",
    "'ᆻ': 61, 'ᆼ': 62, 'ᆽ': 63, 'ᆾ': 64, 'ᆿ': 65, 'ᇀ': 66, 'ᇁ': 67, 'ᇂ': 68, '!': 69, \"'\": 70, \n",
    "'(': 71, ')': 72, ',': 73, '-': 74, '.': 75, ':': 76, ';': 77, '?': 78, ' ': 79\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants from \"ko_dictionary.py\"\n",
    "# coding: utf-8\n",
    "\n",
    "etc_dictionary = {\n",
    "        '2 30대': '이삼십대',\n",
    "        '20~30대': '이삼십대',\n",
    "        '20, 30대': '이십대 삼십대',\n",
    "        '1+1': '원플러스원',\n",
    "        '3에서 6개월인': '3개월에서 육개월인',\n",
    "}\n",
    "\n",
    "english_dictionary = {\n",
    "        'Devsisters': '데브시스터즈',\n",
    "        'track': '트랙',\n",
    "\n",
    "        # krbook\n",
    "        'LA': '엘에이',\n",
    "        'LG': '엘지',\n",
    "        'KOREA': '코리아',\n",
    "        'JSA': '제이에스에이',\n",
    "        'PGA': '피지에이',\n",
    "        'GA': '지에이',\n",
    "        'idol': '아이돌',\n",
    "        'KTX': '케이티엑스',\n",
    "        'AC': '에이씨',\n",
    "        'DVD': '디비디',\n",
    "        'US': '유에스',\n",
    "        'CNN': '씨엔엔',\n",
    "        'LPGA': '엘피지에이',\n",
    "        'P': '피',\n",
    "        'L': '엘',\n",
    "        'T': '티',\n",
    "        'B': '비',\n",
    "        'C': '씨',\n",
    "        'BIFF': '비아이에프에프',\n",
    "        'GV': '지비',\n",
    "\n",
    "        # JTBC\n",
    "        'IT': '아이티',\n",
    "        'IQ': '아이큐',\n",
    "        'JTBC': '제이티비씨',\n",
    "        'trickle down effect': '트리클 다운 이펙트',\n",
    "        'trickle up effect': '트리클 업 이펙트',\n",
    "        'down': '다운',\n",
    "        'up': '업',\n",
    "        'FCK': '에프씨케이',\n",
    "        'AP': '에이피',\n",
    "        'WHERETHEWILDTHINGSARE': '',\n",
    "        'Rashomon Effect': '',\n",
    "        'O': '오',\n",
    "        'OO': '오오',\n",
    "        'B': '비',\n",
    "        'GDP': '지디피',\n",
    "        'CIPA': '씨아이피에이',\n",
    "        'YS': '와이에스',\n",
    "        'Y': '와이',\n",
    "        'S': '에스',\n",
    "        'JTBC': '제이티비씨',\n",
    "        'PC': '피씨',\n",
    "        'bill': '빌',\n",
    "        'Halmuny': '하모니', #####\n",
    "        'X': '엑스',\n",
    "        'SNS': '에스엔에스',\n",
    "        'ability': '어빌리티',\n",
    "        'shy': '',\n",
    "        'CCTV': '씨씨티비',\n",
    "        'IT': '아이티',\n",
    "        'the tenth man': '더 텐쓰 맨', ####\n",
    "        'L': '엘',\n",
    "        'PC': '피씨',\n",
    "        'YSDJJPMB': '', ########\n",
    "        'Content Attitude Timing': '컨텐트 애티튜드 타이밍',\n",
    "        'CAT': '캣',\n",
    "        'IS': '아이에스',\n",
    "        'SNS': '에스엔에스',\n",
    "        'K': '케이',\n",
    "        'Y': '와이',\n",
    "        'KDI': '케이디아이',\n",
    "        'DOC': '디오씨',\n",
    "        'CIA': '씨아이에이',\n",
    "        'PBS': '피비에스',\n",
    "        'D': '디',\n",
    "        'PPropertyPositionPowerPrisonP'\n",
    "        'S': '에스',\n",
    "        'francisco': '프란시스코',\n",
    "        'I': '아이',\n",
    "        'III': '아이아이', ######\n",
    "        'No joke': '노 조크',\n",
    "        'BBK': '비비케이',\n",
    "        'LA': '엘에이',\n",
    "        'Don': '',\n",
    "        't worry be happy': ' 워리 비 해피',\n",
    "        'NO': '엔오', #####\n",
    "        'it was our sky': '잇 워즈 아워 스카이',\n",
    "        'it is our sky': '잇 이즈 아워 스카이', ####\n",
    "        'NEIS': '엔이아이에스', #####\n",
    "        'IMF': '아이엠에프',\n",
    "        'apology': '어폴로지',\n",
    "        'humble': '험블',\n",
    "        'M': '엠',\n",
    "        'Nowhere Man': '노웨어 맨',\n",
    "        'The Tenth Man': '더 텐쓰 맨',\n",
    "        'PBS': '피비에스',\n",
    "        'BBC': '비비씨',\n",
    "        'MRJ': '엠알제이',\n",
    "        'CCTV': '씨씨티비',\n",
    "        'Pick me up': '픽 미 업',\n",
    "        'DNA': '디엔에이',\n",
    "        'UN': '유엔',\n",
    "        'STOP': '스탑', #####\n",
    "        'PRESS': '프레스', #####\n",
    "        'not to be': '낫 투비',\n",
    "        'Denial': '디나이얼',\n",
    "        'G': '지',\n",
    "        'IMF': '아이엠에프',\n",
    "        'GDP': '지디피',\n",
    "        'JTBC': '제이티비씨',\n",
    "        'Time flies like an arrow': '타임 플라이즈 라이크 언 애로우',\n",
    "        'DDT': '디디티',\n",
    "        'AI': '에이아이',\n",
    "        'Z': '제트',\n",
    "        'OECD': '오이씨디',\n",
    "        'N': '앤',\n",
    "        'A': '에이',\n",
    "        'MB': '엠비',\n",
    "        'EH': '이에이치',\n",
    "        'IS': '아이에스',\n",
    "        'TV': '티비',\n",
    "        'MIT': '엠아이티',\n",
    "        'KBO': '케이비오',\n",
    "        'I love America': '아이 러브 아메리카',\n",
    "        'SF': '에스에프',\n",
    "        'Q': '큐',\n",
    "        'KFX': '케이에프엑스',\n",
    "        'PM': '피엠',\n",
    "        'Prime Minister': '프라임 미니스터',\n",
    "        'Swordline': '스워드라인',\n",
    "        'TBS': '티비에스',\n",
    "        'DDT': '디디티',\n",
    "        'CS': '씨에스',\n",
    "        'Reflecting Absence': '리플렉팅 앱센스',\n",
    "        'PBS': '피비에스',\n",
    "        'Drum being beaten by everyone': '드럼 빙 비튼 바이 에브리원',\n",
    "        'negative pressure': '네거티브 프레셔',\n",
    "        'F': '에프',\n",
    "        'KIA': '기아',\n",
    "        'FTA': '에프티에이',\n",
    "        'Que sais-je': '',\n",
    "        'UFC': '유에프씨',\n",
    "        'P': '피',\n",
    "        'DJ': '디제이',\n",
    "        'Chaebol': '채벌',\n",
    "        'BBC': '비비씨',\n",
    "        'OECD': '오이씨디',\n",
    "        'BC': '삐씨',\n",
    "        'C': '씨',\n",
    "        'B': '씨',\n",
    "        'KY': '케이와이',\n",
    "        'K': '케이',\n",
    "        'CEO': '씨이오',\n",
    "        'YH': '와이에치',\n",
    "        'IS': '아이에스',\n",
    "        'who are you': '후 얼 유',\n",
    "        'Y': '와이',\n",
    "        'The Devils Advocate': '더 데빌즈 어드보카트',\n",
    "        'YS': '와이에스',\n",
    "        'so sorry': '쏘 쏘리',\n",
    "        'Santa': '산타',\n",
    "        'Big Endian': '빅 엔디안',\n",
    "        'Small Endian': '스몰 엔디안',\n",
    "        'Oh Captain My Captain': '오 캡틴 마이 캡틴',\n",
    "        'AIB': '에이아이비',\n",
    "        'K': '케이',\n",
    "        'PBS': '피비에스',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants from \"__init__.py\"\n",
    "# Mappings from symbol to numeric ID and vice versa (Korean characters disabled in 'main()'):\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}   # 80개\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "isEn = False\n",
    "\n",
    "# Regular expression matching text enclosed in curly braces:\n",
    "_curly_re = re.compile(r'(.*?)\\{(.+?)\\}(.*)')\n",
    "\n",
    "puncuation_table = str.maketrans({key: None for key in string.punctuation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for \"cleaners.py\"\n",
    "# Code based on https://github.com/keithito/tacotron/blob/master/text/cleaners.py\n",
    "'''\n",
    "Cleaners are transformations that run over the input text at both training and eval time.\n",
    "\n",
    "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
    "hyperparameter. Some cleaners are English-specific. You'll typically want to use:\n",
    "    1. \"english_cleaners\" for English text\n",
    "    2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
    "         the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
    "    3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
    "         the symbols in symbols.py to match your data).\n",
    "'''\n",
    "\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r'\\s+')\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "    ('mrs', 'misess'),\n",
    "    ('mr', 'mister'),\n",
    "    ('dr', 'doctor'),\n",
    "    ('st', 'saint'),\n",
    "    ('co', 'company'),\n",
    "    ('jr', 'junior'),\n",
    "    ('maj', 'major'),\n",
    "    ('gen', 'general'),\n",
    "    ('drs', 'doctors'),\n",
    "    ('rev', 'reverend'),\n",
    "    ('lt', 'lieutenant'),\n",
    "    ('hon', 'honorable'),\n",
    "    ('sgt', 'sergeant'),\n",
    "    ('capt', 'captain'),\n",
    "    ('esq', 'esquire'),\n",
    "    ('ltd', 'limited'),\n",
    "    ('col', 'colonel'),\n",
    "    ('ft', 'fort'),\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants from \"en_numbers.py\"\n",
    "_inflect = inflect.engine()\n",
    "_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n",
    "_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n",
    "_pounds_re = re.compile(r'£([0-9\\,]*[0-9]+)')\n",
    "_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n",
    "_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n",
    "_number_re = re.compile(r'[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions from \"wavloader.py\"\n",
    "def create_dataloader(hp, args, train):\n",
    "    if args.tts:\n",
    "        dataset = AudioTextDataset(hp, args, train)\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=train,\n",
    "            num_workers=hp.train.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=TextCollate()\n",
    "        )\n",
    "    else:\n",
    "        dataset = AudioOnlyDataset(hp, args, train)\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=train,\n",
    "            num_workers=hp.train.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=AudioCollate()\n",
    "        )\n",
    "\n",
    "class AudioOnlyDataset(Dataset):\n",
    "    def __init__(self, hp, args, train):\n",
    "        self.hp = hp\n",
    "        self.args = args\n",
    "        self.train = train\n",
    "        self.data = hp.data.path\n",
    "        self.melgen = MelGen(hp)\n",
    "        self.tierutil = TierUtil(hp)\n",
    "\n",
    "        # this will search all files within hp.data.path\n",
    "        self.file_list = glob.glob(\n",
    "            os.path.join(hp.data.path, '**', hp.data.extension),\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        random.seed(123)\n",
    "        random.shuffle(self.file_list)\n",
    "        if train:\n",
    "            self.file_list = self.file_list[:int(0.95 * len(self.file_list))]\n",
    "        else:\n",
    "            self.file_list = self.file_list[int(0.95 * len(self.file_list)):]\n",
    "\n",
    "        self.wavlen = int(hp.audio.sr * hp.audio.duration)\n",
    "        self.tier = self.args.tier\n",
    "\n",
    "        self.melgen = MelGen(hp)\n",
    "        self.tierutil = TierUtil(hp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav = read_wav_np(self.file_list[idx], sample_rate=self.hp.audio.sr)\n",
    "        # wav = cut_wav(self.wavlen, wav)\n",
    "        mel = self.melgen.get_normalized_mel(wav)\n",
    "        source, target = self.tierutil.cut_divide_tiers(mel, self.tier)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, hp, args, train):\n",
    "        self.hp = hp\n",
    "        self.args = args\n",
    "        self.train = train\n",
    "        self.data = hp.data.path\n",
    "        self.melgen = MelGen(hp)\n",
    "        self.tierutil = TierUtil(hp)\n",
    "\n",
    "        # this will search all files within hp.data.path\n",
    "        self.root_dir = hp.data.path\n",
    "        self.dataset = []\n",
    "        if hp.data.name == 'KSS':\n",
    "            with open(os.path.join(self.root_dir, 'transcript.v.1.3.txt'), 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "                for line in tqdm(lines):\n",
    "                    wav_name, _, _, text, length, _ = line.split('|')\n",
    "\n",
    "                    wav_path = os.path.join(self.root_dir, 'kss', wav_name)\n",
    "                    duraton = float(length)\n",
    "                    if duraton < hp.audio.duration:\n",
    "                        self.dataset.append((wav_path, text))\n",
    "\n",
    "                # if len(self.dataset) > 100: break\n",
    "        elif hp.data.name == 'Blizzard':\n",
    "            with open(os.path.join(self.root_dir, 'prompts.gui'), 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "                filenames = lines[::3]\n",
    "                sentences = lines[1::3]\n",
    "                for filename, sentence in tqdm(zip(filenames, sentences), total=len(filenames)):\n",
    "                    wav_path = os.path.join(self.root_dir, 'wavn', filename + '.wav')\n",
    "                    length = get_length(wav_path, hp.audio.sr)\n",
    "                    if length < hp.audio.duration:\n",
    "                        self.dataset.append((wav_path, sentence))\n",
    "\n",
    "        elif hp.data.name == 'Trump':\n",
    "            with open(os.path.join(self.root_dir, 'prompts.gui'), 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "                filenames = lines[::3]\n",
    "                sentences = lines[1::3]\n",
    "                for filename, sentence in tqdm(zip(filenames, sentences), total=len(filenames)):\n",
    "                    wav_path = os.path.join(self.root_dir, filename)\n",
    "                    length = get_length(wav_path, hp.audio.sr)\n",
    "                    if length < hp.audio.duration:\n",
    "                        self.dataset.append((wav_path, sentence))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        random.seed(123)\n",
    "        random.shuffle(self.dataset)\n",
    "        if train:\n",
    "            self.dataset = self.dataset[:int(0.95 * len(self.dataset))]\n",
    "        else:\n",
    "            self.dataset = self.dataset[int(0.95 * len(self.dataset)):]\n",
    "\n",
    "        self.wavlen = int(hp.audio.sr * hp.audio.duration)\n",
    "        self.tier = self.args.tier\n",
    "\n",
    "        self.melgen = MelGen(hp)\n",
    "        self.tierutil = TierUtil(hp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][1]\n",
    "        if self.hp.data.name == 'KSS':\n",
    "            seq = text_to_sequence(text)\n",
    "        elif self.hp.data.name == 'Blizzard':\n",
    "            seq = process_blizzard(text)\n",
    "        elif self.hp.data.name == 'Trump':\n",
    "            seq = process_trump(text)\n",
    "\n",
    "        wav = read_wav_np(self.dataset[idx][0], sample_rate=self.hp.audio.sr)\n",
    "        # wav = cut_wav(self.wavlen, wav)\n",
    "        mel = self.melgen.get_normalized_mel(wav)\n",
    "        source, target = self.tierutil.cut_divide_tiers(mel, self.tier)\n",
    "\n",
    "        return seq, source, target\n",
    "\n",
    "class TextCollate():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        seq = [torch.from_numpy(x[0]).long() for x in batch]\n",
    "        text_lengths = torch.LongTensor([x.shape[0] for x in seq])\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        seq_padded = torch.nn.utils.rnn.pad_sequence(seq, batch_first=True)\n",
    "\n",
    "        audio_lengths = torch.LongTensor([x[1].shape[1] for x in batch])\n",
    "        source_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.from_numpy(x[1].T) for x in batch],\n",
    "            batch_first=True\n",
    "        ).transpose(1, 2)\n",
    "        target_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.from_numpy(x[2].T) for x in batch],\n",
    "            batch_first=True\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        return seq_padded, text_lengths, source_padded, target_padded, audio_lengths\n",
    "\n",
    "class AudioCollate():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        audio_lengths = torch.LongTensor([x[0].shape[1] for x in batch])\n",
    "        source_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.from_numpy(x[0].T) for x in batch],\n",
    "            batch_first=True\n",
    "        ).transpose(1, 2)\n",
    "        target_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.from_numpy(x[1].T) for x in batch],\n",
    "            batch_first=True\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        return source_padded, target_padded, audio_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions from \"en_numbers.py\"\n",
    "def _remove_commas(m):\n",
    "  return m.group(1).replace(',', '')\n",
    "\n",
    "\n",
    "def _expand_decimal_point(m):\n",
    "  return m.group(1).replace('.', ' point ')\n",
    "\n",
    "\n",
    "def _expand_dollars(m):\n",
    "  match = m.group(1)\n",
    "  parts = match.split('.')\n",
    "  if len(parts) > 2:\n",
    "    return match + ' dollars'  # Unexpected format\n",
    "  dollars = int(parts[0]) if parts[0] else 0\n",
    "  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "  if dollars and cents:\n",
    "    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "    cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n",
    "  elif dollars:\n",
    "    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "    return '%s %s' % (dollars, dollar_unit)\n",
    "  elif cents:\n",
    "    cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "    return '%s %s' % (cents, cent_unit)\n",
    "  else:\n",
    "    return 'zero dollars'\n",
    "\n",
    "\n",
    "def _expand_ordinal(m):\n",
    "  return _inflect.number_to_words(m.group(0))\n",
    "\n",
    "\n",
    "def _expand_number(m):\n",
    "  num = int(m.group(0))\n",
    "  if num > 1000 and num < 3000:\n",
    "    if num == 2000:\n",
    "      return 'two thousand'\n",
    "    elif num > 2000 and num < 2010:\n",
    "      return 'two thousand ' + _inflect.number_to_words(num % 100)\n",
    "    elif num % 100 == 0:\n",
    "      return _inflect.number_to_words(num // 100) + ' hundred'\n",
    "    else:\n",
    "      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n",
    "  else:\n",
    "    return _inflect.number_to_words(num, andword='')\n",
    "\n",
    "\n",
    "def normalize_numbers(text):\n",
    "  text = re.sub(_comma_number_re, _remove_commas, text)\n",
    "  text = re.sub(_pounds_re, r'\\1 pounds', text)\n",
    "  text = re.sub(_dollars_re, _expand_dollars, text)\n",
    "  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
    "  text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
    "  text = re.sub(_number_re, _expand_number, text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions from \"cleaners.py\"\n",
    "def korean_cleaners(text):\n",
    "    '''Pipeline for Korean text, including number and abbreviation expansion.'''\n",
    "    text = tokenize(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    for regex, replacement in _abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "    return normalize_numbers(text)\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "    return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "    '''Converts to ascii, existed in keithito but deleted in carpedm20'''\n",
    "    return unidecode(text)\n",
    "    \n",
    "\n",
    "def basic_cleaners(text):\n",
    "    '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transliteration_cleaners(text):\n",
    "    '''Pipeline for non-English text that transliterates to ASCII.'''\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def english_cleaners(text):\n",
    "    '''Pipeline for English text, including number and abbreviation expansion.'''\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = expand_abbreviations(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions from \"__init__.py\"\n",
    "def convert_to_en_symbols():\n",
    "    '''Converts built-in korean symbols to english, to be used for english training\n",
    "    \n",
    "'''\n",
    "    global _symbol_to_id, _id_to_symbol, isEn, en_symbols\n",
    "    if not isEn:\n",
    "        print(\" [!] Converting to english mode\")\n",
    "    en_symbols = SYMBOLS + NUMBERS + PAD + EOS + PUNC + SPACE\n",
    "    _symbol_to_id = {s: i for i, s in enumerate(en_symbols)}\n",
    "    _id_to_symbol = {i: s for i, s in enumerate(en_symbols)}\n",
    "    print(_symbol_to_id)\n",
    "    isEn = True\n",
    "\n",
    "def remove_puncuations(text):\n",
    "    return text.translate(puncuation_table)\n",
    "\n",
    "def text_to_sequence(text, cleaner_names=['korean_cleaners'], as_token=False):    \n",
    "    return _text_to_sequence(text, cleaner_names, as_token)\n",
    "\n",
    "def _text_to_sequence(text, cleaner_names, as_token):\n",
    "    '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "\n",
    "        The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
    "        in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "\n",
    "        Args:\n",
    "            text: string to convert to a sequence\n",
    "            cleaner_names: names of the cleaner functions to run the text through\n",
    "\n",
    "        Returns:\n",
    "            List of integers corresponding to the symbols in the text\n",
    "    '''\n",
    "    sequence = []\n",
    "\n",
    "    # Check for curly braces and treat their contents as ARPAbet:\n",
    "    while len(text):\n",
    "        m = _curly_re.match(text)\n",
    "        if not m:\n",
    "            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
    "            break\n",
    "        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n",
    "        sequence += _arpabet_to_sequence(m.group(2))\n",
    "        text = m.group(3)\n",
    "\n",
    "    # Append EOS token\n",
    "    sequence.append(_symbol_to_id[EOS])  # [14, 29, 45, 2, 27, 62, 20, 21, 4, 39, 45, 1]\n",
    "\n",
    "    if as_token:\n",
    "        return sequence_to_text(sequence, combine_jamo=True)\n",
    "    else:\n",
    "        return np.array(sequence, dtype=np.int32)\n",
    "\n",
    "\n",
    "def sequence_to_text(sequence, skip_eos_and_pad=False, combine_jamo=False):\n",
    "    '''Converts a sequence of IDs back to a string'''\n",
    "        \n",
    "    result = ''\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            # Enclose ARPAbet back in curly braces:\n",
    "            if len(s) > 1 and s[0] == '@':\n",
    "                s = '{%s}' % s[1:]\n",
    "\n",
    "            if not skip_eos_and_pad or s not in [EOS, PAD]:\n",
    "                result += s\n",
    "\n",
    "    result = result.replace('}{', ' ')\n",
    "\n",
    "    if combine_jamo:\n",
    "        return jamo_to_korean(result)\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "def _clean_text(text, cleaner_names):\n",
    "    \n",
    "    for name in cleaner_names:\n",
    "        cleaner = getattr(cleaners, name)\n",
    "        if not cleaner:\n",
    "            raise Exception('Unknown cleaner: %s' % name)\n",
    "        text = cleaner(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def _symbols_to_sequence(symbols):\n",
    "    return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n",
    "\n",
    "\n",
    "def _arpabet_to_sequence(text):\n",
    "    return _symbols_to_sequence(['@' + s for s in text.split()])\n",
    "\n",
    "\n",
    "def _should_keep_symbol(s):\n",
    "    return s in _symbol_to_id and s is not '_' and s is not '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions in \"korean.py\"\n",
    "def is_lead(char):\n",
    "    return char in JAMO_LEADS\n",
    "\n",
    "def is_vowel(char):\n",
    "    return char in JAMO_VOWELS\n",
    "\n",
    "def is_tail(char):\n",
    "    return char in JAMO_TAILS\n",
    "\n",
    "def get_mode(char):\n",
    "    if is_lead(char):\n",
    "        return 0\n",
    "    elif is_vowel(char):\n",
    "        return 1\n",
    "    elif is_tail(char):\n",
    "        return 2\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def _get_text_from_candidates(candidates):\n",
    "    if len(candidates) == 0:\n",
    "        return \"\"\n",
    "    elif len(candidates) == 1:\n",
    "        return _jamo_char_to_hcj(candidates[0])\n",
    "    else:\n",
    "        return j2h(**dict(zip([\"lead\", \"vowel\", \"tail\"], candidates)))\n",
    "\n",
    "def jamo_to_korean(text):\n",
    "    text = h2j(text)\n",
    "\n",
    "    idx = 0\n",
    "    new_text = \"\"\n",
    "    candidates = []\n",
    "\n",
    "    while True:\n",
    "        if idx >= len(text):\n",
    "            new_text += _get_text_from_candidates(candidates)\n",
    "            break\n",
    "\n",
    "        char = text[idx]\n",
    "        mode = get_mode(char)\n",
    "\n",
    "        if mode == 0:\n",
    "            new_text += _get_text_from_candidates(candidates)\n",
    "            candidates = [char]\n",
    "        elif mode == -1:\n",
    "            new_text += _get_text_from_candidates(candidates)\n",
    "            new_text += char\n",
    "            candidates = []\n",
    "        else:\n",
    "            candidates.append(char)\n",
    "\n",
    "        idx += 1\n",
    "    return new_text\n",
    "\n",
    "def compare_sentence_with_jamo(text1, text2):\n",
    "    return h2j(text1) != h2j(text2)\n",
    "\n",
    "def tokenize(text, as_id=False):\n",
    "    # jamo package에 있는 hangul_to_jamo를 이용하여 한글 string을 초성/중성/종성으로 나눈다.\n",
    "    text = normalize(text)\n",
    "    tokens = list(hangul_to_jamo(text)) # '존경하는'  --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
    "\n",
    "    if as_id:\n",
    "        return [char_to_id[token] for token in tokens] + [char_to_id[EOS]]\n",
    "    else:\n",
    "        return [token for token in tokens] + [EOS]\n",
    "\n",
    "def tokenizer_fn(iterator):\n",
    "    return (token for x in iterator for token in tokenize(x, as_id=False))\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.strip()\n",
    "\n",
    "    text = re.sub('\\(\\d+일\\)', '', text)\n",
    "    text = re.sub('\\([⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]+\\)', '', text)\n",
    "\n",
    "    text = normalize_with_dictionary(text, etc_dictionary)\n",
    "    text = normalize_english(text)\n",
    "    text = re.sub('[a-zA-Z]+', normalize_upper, text)\n",
    "\n",
    "    text = normalize_quote(text)\n",
    "    text = normalize_number(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_with_dictionary(text, dic):\n",
    "    if any(key in text for key in dic.keys()):\n",
    "        pattern = re.compile('|'.join(re.escape(key) for key in dic.keys()))\n",
    "        return pattern.sub(lambda x: dic[x.group()], text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def normalize_english(text):\n",
    "    def fn(m):\n",
    "        word = m.group()\n",
    "        if word in english_dictionary:\n",
    "            return english_dictionary.get(word)\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    text = re.sub(\"([A-Za-z]+)\", fn, text)\n",
    "    return text\n",
    "\n",
    "def normalize_upper(text):\n",
    "    text = text.group(0)\n",
    "\n",
    "    if all([char.isupper() for char in text]):\n",
    "        return \"\".join(upper_to_kor[char] for char in text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def normalize_quote(text):\n",
    "    def fn(found_text):\n",
    "        from nltk import sent_tokenize # NLTK doesn't along with multiprocessing\n",
    "\n",
    "        found_text = found_text.group()\n",
    "        unquoted_text = found_text[1:-1]\n",
    "\n",
    "        sentences = sent_tokenize(unquoted_text)\n",
    "        return \" \".join([\"'{}'\".format(sent) for sent in sentences])\n",
    "\n",
    "    return re.sub(quote_checker, fn, text)\n",
    "\n",
    "def normalize_number(text):\n",
    "    text = normalize_with_dictionary(text, unit_to_kor1)\n",
    "    text = normalize_with_dictionary(text, unit_to_kor2)\n",
    "    text = re.sub(number_checker + count_checker,\n",
    "            lambda x: number_to_korean(x, True), text)\n",
    "    text = re.sub(number_checker,\n",
    "            lambda x: number_to_korean(x, False), text)\n",
    "    return text\n",
    "\n",
    "def number_to_korean(num_str, is_count=False):\n",
    "    if is_count:\n",
    "        num_str, unit_str = num_str.group(1), num_str.group(2)\n",
    "    else:\n",
    "        num_str, unit_str = num_str.group(), \"\"\n",
    "    \n",
    "    num_str = num_str.replace(',', '')\n",
    "    num = ast.literal_eval(num_str)\n",
    "\n",
    "    if num == 0:\n",
    "        return \"영\"\n",
    "\n",
    "    check_float = num_str.split('.')\n",
    "    if len(check_float) == 2:\n",
    "        digit_str, float_str = check_float\n",
    "    elif len(check_float) >= 3:\n",
    "        raise Exception(\" [!] Wrong number format\")\n",
    "    else:\n",
    "        digit_str, float_str = check_float[0], None\n",
    "\n",
    "    if is_count and float_str is not None:\n",
    "        raise Exception(\" [!] `is_count` and float number does not fit each other\")\n",
    "\n",
    "    digit = int(digit_str)\n",
    "\n",
    "    if digit_str.startswith(\"-\"):\n",
    "        digit, digit_str = abs(digit), str(abs(digit))\n",
    "\n",
    "    kor = \"\"\n",
    "    size = len(str(digit))\n",
    "    tmp = []\n",
    "\n",
    "    for i, v in enumerate(digit_str, start=1):\n",
    "        v = int(v)\n",
    "\n",
    "        if v != 0:\n",
    "            if is_count:\n",
    "                tmp += count_to_kor1[v]\n",
    "            else:\n",
    "                tmp += num_to_kor1[v]\n",
    "\n",
    "            tmp += num_to_kor3[(size - i) % 4]\n",
    "\n",
    "        if (size - i) % 4 == 0 and len(tmp) != 0:\n",
    "            kor += \"\".join(tmp)\n",
    "            tmp = []\n",
    "            kor += num_to_kor2[int((size - i) / 4)]\n",
    "\n",
    "    if is_count:\n",
    "        if kor.startswith(\"한\") and len(kor) > 1:\n",
    "            kor = kor[1:]\n",
    "\n",
    "        if any(word in kor for word in count_tenth_dict):\n",
    "            kor = re.sub(\n",
    "                    '|'.join(count_tenth_dict.keys()),\n",
    "                    lambda x: count_tenth_dict[x.group()], kor)\n",
    "\n",
    "    if not is_count and kor.startswith(\"일\") and len(kor) > 1:\n",
    "        kor = kor[1:]\n",
    "\n",
    "    if float_str is not None:\n",
    "        kor += \"쩜 \"\n",
    "        kor += re.sub('\\d', lambda x: num_to_kor[x.group()], float_str)\n",
    "\n",
    "    if num_str.startswith(\"+\"):\n",
    "        kor = \"플러스 \" + kor\n",
    "    elif num_str.startswith(\"-\"):\n",
    "        kor = \"마이너스 \" + kor\n",
    "\n",
    "    return kor + unit_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test driver function for \"korean.py\"\n",
    "if __name__ == \"__main__\":\n",
    "    def test_normalize(text):\n",
    "        print(text)\n",
    "        print(normalize(text))\n",
    "        print(\"=\"*30)\n",
    "\n",
    "    test_normalize(\"JTBC는 JTBCs를 DY는 A가 Absolute\")\n",
    "    test_normalize(\"오늘(13일) 3,600마리 강아지가\")\n",
    "    test_normalize(\"60.3%\")\n",
    "    test_normalize('\"저돌\"(猪突) 입니다.')\n",
    "    test_normalize('비대위원장이 지난 1월 이런 말을 했습니다. “난 그냥 산돼지처럼 돌파하는 스타일이다”')\n",
    "    test_normalize(\"지금은 -12.35%였고 종류는 5가지와 19가지, 그리고 55가지였다\")\n",
    "    test_normalize(\"JTBC는 TH와 K 양이 2017년 9월 12일 오후 12시에 24살이 된다\")\n",
    "    print(list(hangul_to_jamo(list(hangul_to_jamo('비대위원장이 지난 1월 이런 말을 했습니다? “난 그냥 산돼지처럼 돌파하는 스타일이다”')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions in \"utils.py\"\n",
    "def get_length(wavpath, sample_rate):\n",
    "    audio = audiosegment.from_file(wavpath).resample(sample_rate_Hz=sample_rate)\n",
    "    return audio.duration_seconds\n",
    "\n",
    "def process_blizzard(text: str):\n",
    "    text = text.replace('@ ', '').replace('# ', '').replace('| ', '') + EOS\n",
    "    seq = [_symbol_to_id[c] for c in text]\n",
    "    return np.array(seq, dtype=np.int32)\n",
    "\n",
    "def process_trump(text: str):\n",
    "    text = text.replace('@ ', '').replace('# ', '').replace('| ', '') + EOS\n",
    "    seq = [_symbol_to_id[c] for c in text]\n",
    "    return np.array(seq, dtype=np.int32)\n",
    "\n",
    "def get_commit_hash():\n",
    "    message = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"])\n",
    "    return message.strip().decode('utf-8')\n",
    "\n",
    "def read_wav_np(wavpath, sample_rate):\n",
    "    audio = audiosegment.from_file(wavpath).resample(sample_rate_Hz=sample_rate)\n",
    "    wav = audio.to_numpy_array()\n",
    "    \n",
    "    if len(wav.shape) == 2:\n",
    "        wav = wav.T.flatten()\n",
    "    \n",
    "    if wav.dtype == np.int16:\n",
    "        wav = wav / 32768.0\n",
    "    elif wav.dtype == np.int32:\n",
    "        wav = wav / 2147483648.0\n",
    "    elif wav.dtype == np.uint8:\n",
    "        wav = (wav - 128) / 128.0\n",
    "    \n",
    "    wav = wav.astype(np.float32)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def cut_wav(L, wav):\n",
    "    samples = len(wav)\n",
    "    if samples < L:\n",
    "        wav = np.pad(wav, (0, L - samples), 'constant', constant_values=0.0)\n",
    "    else:\n",
    "        start = random.randint(0, samples - L)\n",
    "        wav = wav[start:start + L]\n",
    "\n",
    "    return wav\n",
    "\n",
    "\n",
    "def norm_wav(wav):\n",
    "    assert isinstance(wav, np.ndarray) and len(wav.shape)==1, 'Wav file should be 1D numpy array'\n",
    "    return wav / np.max( np.abs(wav) )\n",
    "\n",
    "\n",
    "def trim_wav(wav, threshold=0.01):\n",
    "    assert isinstance(wav, np.ndarray) and len(wav.shape)==1, 'Wav file should be 1D numpy array'\n",
    "    cut = np.where((abs(wav)>threshold))[0]\n",
    "    wav = wav[cut[0]:(cut[-1]+1)]\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions of \"gmm.py\"\n",
    "def get_pi_indices(pi):\n",
    "    cumsum = torch.cumsum(pi.cpu(), dim=-1)\n",
    "    rand = torch.rand(pi.shape[:-1] + (1,))\n",
    "    indices = (cumsum < rand).sum(dim=-1)\n",
    "    return indices.flatten().detach().numpy()\n",
    "\n",
    "def sample_gmm(mu, std, pi):\n",
    "    std = std.exp()\n",
    "    pi = pi.softmax(dim=-1)\n",
    "    indices = get_pi_indices(pi)\n",
    "    mu = mu.reshape(-1, mu.shape[-1])\n",
    "    mu = mu[np.arange(mu.shape[0]), indices].reshape(std.shape[:-1])\n",
    "    std = std.reshape(-1, std.shape[-1])\n",
    "    std = std[np.arange(std.shape[0]), indices].reshape(mu.shape)\n",
    "    return torch.normal(mu, std).reshape_as(mu).clamp(0.0, 1.0).to(mu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train function of \"train.py\"\n",
    "def train(args, pt_dir, chkpt_path, trainloader, testloader, writer, logger, hp, hp_str):\n",
    "    if args.tts:\n",
    "        model = TTS(\n",
    "            hp=hp,\n",
    "            freq=hp.audio.n_mels // f_div[hp.model.tier+1] * f_div[args.tier],\n",
    "            layers=hp.model.layers[args.tier-1]\n",
    "        )\n",
    "    else:\n",
    "        model = Tier(\n",
    "            hp=hp,\n",
    "            freq=hp.audio.n_mels // f_div[hp.model.tier+1] * f_div[args.tier],\n",
    "            layers=hp.model.layers[args.tier-1],\n",
    "            tierN=args.tier\n",
    "        )\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    melgen = MelGen(hp)\n",
    "    tierutil = TierUtil(hp)\n",
    "    criterion = GMMLoss()\n",
    "\n",
    "    if hp.train.optimizer == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(\n",
    "            model.parameters(), \n",
    "            lr=hp.train.rmsprop.lr, \n",
    "            momentum=hp.train.rmsprop.momentum\n",
    "        )\n",
    "    elif hp.train.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=hp.train.adam.lr\n",
    "        )\n",
    "    elif hp.train.optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), \n",
    "            lr=hp.train.sgd.lr\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"%s optimizer not supported yet\" % hp.train.optimizer)\n",
    "\n",
    "    # githash = get_commit_hash()\n",
    "\n",
    "    init_epoch = -1\n",
    "    step = 0\n",
    "\n",
    "    if chkpt_path is not None:\n",
    "        logger.info(\"Resuming from checkpoint: %s\" % chkpt_path)\n",
    "        checkpoint = torch.load(chkpt_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        step = checkpoint['step']\n",
    "        init_epoch = checkpoint['epoch']\n",
    "\n",
    "        if hp_str != checkpoint['hp_str']:\n",
    "            logger.warning(\"New hparams is different from checkpoint. Will use new.\")\n",
    "\n",
    "        # if githash != checkpoint['githash']:\n",
    "        #     logger.warning(\"Code might be different: git hash is different.\")\n",
    "        #     logger.warning(\"%s -> %s\" % (checkpoint['githash'], githash))\n",
    "\n",
    "        # githash = checkpoint['githash']\n",
    "    else:\n",
    "        logger.info(\"Starting new training run.\")\n",
    "\n",
    "    # use this only if input size is always consistent.\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    try:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss_sum = 0\n",
    "        for epoch in itertools.count(init_epoch + 1):\n",
    "            loader = tqdm(trainloader, desc='Train data loader', dynamic_ncols=True)\n",
    "            for input_tuple in loader:\n",
    "                if args.tts:\n",
    "                    seq, text_lengths, source, target, audio_lengths = input_tuple\n",
    "                    mu, std, pi, _ = model(\n",
    "                        source.cuda(non_blocking=True),\n",
    "                        seq.cuda(non_blocking=True),\n",
    "                        text_lengths.cuda(non_blocking=True),\n",
    "                        audio_lengths.cuda(non_blocking=True)\n",
    "                    )\n",
    "                else:\n",
    "                    source, target, audio_lengths = input_tuple\n",
    "                    mu, std, pi = model(\n",
    "                        source.cuda(non_blocking=True),\n",
    "                        audio_lengths.cuda(non_blocking=True)\n",
    "                    )\n",
    "                loss = criterion(\n",
    "                    target.cuda(non_blocking=True),\n",
    "                    mu, std, pi,\n",
    "                    audio_lengths.cuda(non_blocking=True)\n",
    "                )\n",
    "                step += 1\n",
    "                (loss / hp.train.update_interval).backward()\n",
    "                loss_sum += loss.item() / hp.train.update_interval\n",
    "\n",
    "                if step % hp.train.update_interval == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    if step % hp.log.summary_interval == 0:\n",
    "                        writer.log_training(loss_sum, step)\n",
    "                        loader.set_description(\"Loss %.04f at step %d\" % (loss_sum, step))\n",
    "                    loss_sum = 0\n",
    "\n",
    "                loss = loss.item()\n",
    "                if loss > 1e8 or math.isnan(loss):\n",
    "                    logger.error(\"Loss exploded to %.04f at step %d!\" % (loss, step))\n",
    "                    raise Exception(\"Loss exploded\")\n",
    "\n",
    "            save_path = os.path.join(pt_dir, '%s_%s_tier%d_%03d.pt')\n",
    "            # % (args.name, githash, args.tier, epoch))\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'step': step,\n",
    "                'epoch': epoch,\n",
    "                'hp_str': hp_str,\n",
    "                # 'githash': githash,\n",
    "            }, save_path)\n",
    "            logger.info(\"Saved checkpoint to: %s\" % save_path)\n",
    "\n",
    "            validate(args, model, melgen, tierutil, testloader, criterion, writer, step)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(\"Exiting due to exception: %s\" % e)\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validate function from \"validation.py\"\n",
    "def validate(args, model, melgen, tierutil, testloader, criterion, writer, step):\n",
    "    model.eval()\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    test_loss = []\n",
    "    loader = tqdm(testloader, desc='Testing is in progress', dynamic_ncols=True)\n",
    "    with torch.no_grad():\n",
    "        for input_tuple in loader:\n",
    "            if args.tts:\n",
    "                seq, text_lengths, source, target, audio_lengths = input_tuple\n",
    "                mu, std, pi, alignment = model(\n",
    "                    source.cuda(non_blocking=True),\n",
    "                    seq.cuda(non_blocking=True),\n",
    "                    text_lengths.cuda(non_blocking=True),\n",
    "                    audio_lengths.cuda(non_blocking=True)\n",
    "                )\n",
    "            else:\n",
    "                source, target, audio_lengths = input_tuple\n",
    "                mu, std, pi = model(\n",
    "                    source.cuda(non_blocking=True),\n",
    "                    audio_lengths.cuda(non_blocking=True)\n",
    "                )\n",
    "            loss = criterion(\n",
    "                target.cuda(non_blocking=True),\n",
    "                mu, std, pi,\n",
    "                audio_lengths.cuda(non_blocking=True)\n",
    "            )\n",
    "            test_loss.append(loss)\n",
    "\n",
    "        test_loss = sum(test_loss) / len(test_loss)\n",
    "        audio_length = audio_lengths[0].item()\n",
    "        source = source[0].cpu().detach().numpy()[:, :audio_length]\n",
    "        target = target[0].cpu().detach().numpy()[:, :audio_length]\n",
    "        result = sample_gmm(mu[0], std[0], pi[0]).cpu().detach().numpy()[:, :audio_length]\n",
    "        if args.tts:\n",
    "            alignment = alignment[0].cpu().detach().numpy()[:, :audio_length]\n",
    "        else:\n",
    "            alignment = None\n",
    "        writer.log_validation(test_loss, source, target, result, alignment, step)\n",
    "\n",
    "    model.train()\n",
    "    # torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions from \"hparams.py\"\n",
    "def load_hparam_str(hp_str):\n",
    "    path = os.path.join('temp-restore.yaml')\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(hp_str)\n",
    "    ret = HParam(path)\n",
    "    os.remove(path)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def load_hparam(filename):\n",
    "    stream = open(filename, 'r')\n",
    "    docs = yaml.load_all(stream, Loader=yaml.Loader)\n",
    "    hparam_dict = dict()\n",
    "    for doc in docs:\n",
    "        for k, v in doc.items():\n",
    "            hparam_dict[k] = v\n",
    "    return hparam_dict\n",
    "\n",
    "\n",
    "def merge_dict(user, default):\n",
    "    if isinstance(user, dict) and isinstance(default, dict):\n",
    "        for k, v in default.items():\n",
    "            if k not in user:\n",
    "                user[k] = v\n",
    "            else:\n",
    "                user[k] = merge_dict(user[k], v)\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions from \"plotting.py\"\n",
    "def fig2np(fig):\n",
    "    # save it to a numpy array.\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    data = data.transpose(2, 0, 1)\n",
    "    return data\n",
    "\n",
    "def plot_spectrogram_to_numpy(spectrogram):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    im = ax.imshow(spectrogram, aspect='auto', origin='lower',\n",
    "                   interpolation='none')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Channels')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    data = fig2np(fig)\n",
    "    plt.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes of \"hparams.py\"\n",
    "class Dotdict(dict):\n",
    "    \"\"\"\n",
    "    a dictionary that supports dot notation \n",
    "    as well as dictionary access notation \n",
    "    usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
    "    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
    "    get attributes: d.val2 or d['val2']\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __init__(self, dct=None):\n",
    "        dct = dict() if not dct else dct\n",
    "        for key, value in dct.items():\n",
    "            if hasattr(value, 'keys'):\n",
    "                value = Dotdict(value)\n",
    "            self[key] = value\n",
    "\n",
    "\n",
    "class HParam(Dotdict):\n",
    "\n",
    "    def __init__(self, file):\n",
    "        super(Dotdict, self).__init__()\n",
    "        hp_dict = load_hparam(file)\n",
    "        hp_dotdict = Dotdict(hp_dict)\n",
    "        for k, v in hp_dotdict.items():\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "    __getattr__ = Dotdict.__getitem__\n",
    "    __setattr__ = Dotdict.__setitem__\n",
    "    __delattr__ = Dotdict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MyWriter class from \"writer.py\"\n",
    "class MyWriter(SummaryWriter):\n",
    "    def __init__(self, hp, logdir):\n",
    "        super(MyWriter, self).__init__(logdir)\n",
    "        self.hp = hp\n",
    "\n",
    "    def log_training(self, train_loss, step):\n",
    "        self.add_scalar('train_loss', train_loss, step)\n",
    "        # self.add_histogram('mu', mu, step)\n",
    "        # self.add_histogram('std', std, step)\n",
    "        # self.add_histogram('std_exp', std.exp(), step)\n",
    "        # self.add_histogram('pi', pi, step)\n",
    "        # self.add_histogram('pi_softmax', pi.softmax(dim=3), step)\n",
    "\n",
    "    def log_validation(self, test_loss, source, target, result, alignment, step):\n",
    "        self.add_scalar('test_loss', test_loss, step)\n",
    "        self.add_image('input', plot_spectrogram_to_numpy(source), step)\n",
    "        self.add_image('target', plot_spectrogram_to_numpy(target), step)\n",
    "        self.add_image('result', plot_spectrogram_to_numpy(result), step)\n",
    "        self.add_image('diff', plot_spectrogram_to_numpy(target - result), step)\n",
    "        if alignment is not None:\n",
    "            self.add_image('alignment', plot_spectrogram_to_numpy(alignment.T), step)\n",
    "\n",
    "    def log_sample(self, step):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tier class of \"tier.py\"\n",
    "class Tier(nn.Module):\n",
    "    def __init__(self, hp, freq, layers, tierN):\n",
    "        super(Tier, self).__init__()\n",
    "        num_hidden = hp.model.hidden\n",
    "        self.hp = hp\n",
    "        self.tierN = tierN\n",
    "\n",
    "        if(tierN == 1):\n",
    "            self.W_t_0 = nn.Linear(1, num_hidden)\n",
    "            self.W_f_0 = nn.Linear(1, num_hidden)\n",
    "            self.W_c_0 = nn.Linear(freq, num_hidden)\n",
    "            self.layers = nn.ModuleList([\n",
    "                DelayedRNN(hp) for _ in range(layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.W_t = nn.Linear(1, num_hidden)\n",
    "            self.layers = nn.ModuleList([\n",
    "                UpsampleRNN(hp) for _ in range(layers)\n",
    "            ])\n",
    "\n",
    "        # Gaussian Mixture Model: eq. (2)\n",
    "        self.K = hp.model.gmm\n",
    "        self.pi_softmax = nn.Softmax(dim=3)\n",
    "\n",
    "        # map output to produce GMM parameter eq. (10)\n",
    "        self.W_theta = nn.Linear(num_hidden, 3*self.K)\n",
    "\n",
    "    def forward(self, x, audio_lengths):\n",
    "        # x: [B, M, T] / B=batch, M=mel, T=time\n",
    "        if self.tierN == 1:\n",
    "            h_t = self.W_t_0(F.pad(x, [1, -1]).unsqueeze(-1))\n",
    "            h_f = self.W_f_0(F.pad(x, [0, 0, 1, -1]).unsqueeze(-1))\n",
    "            h_c = self.W_c_0(F.pad(x, [1, -1]).transpose(1, 2))\n",
    "            for layer in self.layers:\n",
    "                h_t, h_f, h_c = layer(h_t, h_f, h_c, audio_lengths)\n",
    "\n",
    "            # h_t, h_f: [B, M, T, D] / D=num_hidden\n",
    "            # h_c: [B, T, D]\n",
    "        else:\n",
    "            h_f = self.W_t(x.unsqueeze(-1))\n",
    "            for layer in self.layers:\n",
    "                h_f = layer(h_f, audio_lengths)\n",
    "\n",
    "        theta_hat = self.W_theta(h_f)\n",
    "\n",
    "        mu = theta_hat[..., :self.K] # eq. (3)\n",
    "        std = theta_hat[..., self.K:2*self.K]\n",
    "        pi = theta_hat[..., 2*self.K:]\n",
    "\n",
    "        return mu, std, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define delayedRNN class of \"rnn.py\"\n",
    "class DelayedRNN(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(DelayedRNN, self).__init__()\n",
    "        self.num_hidden = hp.model.hidden\n",
    "\n",
    "        self.t_delay_RNN_x = nn.LSTM(\n",
    "            input_size=self.num_hidden,\n",
    "            hidden_size=self.num_hidden,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.t_delay_RNN_yz = nn.LSTM(\n",
    "            input_size=self.num_hidden,\n",
    "            hidden_size=self.num_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # use central stack only at initial tier\n",
    "        self.c_RNN = nn.LSTM(\n",
    "            input_size=self.num_hidden,\n",
    "            hidden_size=self.num_hidden,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.f_delay_RNN = nn.LSTM(\n",
    "            input_size=self.num_hidden,\n",
    "            hidden_size=self.num_hidden,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.W_t = nn.Linear(3*self.num_hidden, self.num_hidden)\n",
    "        self.W_c = nn.Linear(self.num_hidden, self.num_hidden)\n",
    "        self.W_f = nn.Linear(self.num_hidden, self.num_hidden)\n",
    "   \n",
    "    def flatten_rnn(self):\n",
    "        self.t_delay_RNN_x.flatten_parameters()\n",
    "        self.t_delay_RNN_yz.flatten_parameters()\n",
    "        self.c_RNN.flatten_parameters()\n",
    "        self.f_delay_RNN.flatten_parameters()\n",
    "\n",
    "    def forward(self, input_h_t, input_h_f, input_h_c, audio_lengths):\n",
    "      \n",
    "        self.flatten_rnn()\n",
    "        # input_h_t, input_h_f: [B, M, T, D]\n",
    "        # input_h_c: [B, T, D]\n",
    "        B, M, T, D = input_h_t.size()\n",
    "\n",
    "        ####### time-delayed stack #######\n",
    "        # Fig. 2(a)-1 can be parallelized by viewing each horizontal line as batch\n",
    "        h_t_x_temp = input_h_t.view(-1, T, D)\n",
    "        h_t_x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            h_t_x_temp,\n",
    "            audio_lengths.unsqueeze(1).repeat(1, M).reshape(-1),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        h_t_x, _ = self.t_delay_RNN_x(h_t_x_packed)\n",
    "        h_t_x, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            h_t_x,\n",
    "            batch_first=True,\n",
    "            total_length=T\n",
    "        )\n",
    "        h_t_x = h_t_x.view(B, M, T, D)\n",
    "\n",
    "        # Fig. 2(a)-2,3 can be parallelized by viewing each vertical line as batch,\n",
    "        # using bi-directional version of GRU\n",
    "        h_t_yz_temp = input_h_t.transpose(1, 2).contiguous() # [B, T, M, D]\n",
    "        h_t_yz_temp = h_t_yz_temp.view(-1, M, D)\n",
    "        h_t_yz, _ = self.t_delay_RNN_yz(h_t_yz_temp)\n",
    "        h_t_yz = h_t_yz.view(B, T, M, 2*D)\n",
    "        h_t_yz = h_t_yz.transpose(1, 2)\n",
    "\n",
    "        h_t_concat = torch.cat((h_t_x, h_t_yz), dim=3)\n",
    "        output_h_t = input_h_t + self.W_t(h_t_concat) # residual connection, eq. (6)\n",
    "\n",
    "        ####### centralized stack #######\n",
    "        h_c_temp = nn.utils.rnn.pack_padded_sequence(\n",
    "            input_h_c,\n",
    "            audio_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        h_c_temp, _ = self.c_RNN(h_c_temp)\n",
    "        h_c_temp, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            h_c_temp,\n",
    "            batch_first=True,\n",
    "            total_length=T\n",
    "        )\n",
    "            \n",
    "        output_h_c = input_h_c + self.W_c(h_c_temp) # residual connection, eq. (11)\n",
    "        h_c_expanded = output_h_c.unsqueeze(1)\n",
    "\n",
    "        ####### frequency-delayed stack #######\n",
    "        h_f_sum = input_h_f + output_h_t + h_c_expanded\n",
    "        h_f_sum = h_f_sum.transpose(1, 2).contiguous() # [B, T, M, D]\n",
    "        h_f_sum = h_f_sum.view(-1, M, D)\n",
    "\n",
    "        h_f_temp, _ = self.f_delay_RNN(h_f_sum)\n",
    "        h_f_temp = h_f_temp.view(B, T, M, D)\n",
    "        h_f_temp = h_f_temp.transpose(1, 2) # [B, M, T, D]\n",
    "        \n",
    "        output_h_f = input_h_f + self.W_f(h_f_temp) # residual connection, eq. (8)\n",
    "\n",
    "        return output_h_t, output_h_f, output_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upsampleRNN of \"upsample.py\"\n",
    "class UpsampleRNN(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(UpsampleRNN, self).__init__()\n",
    "        self.num_hidden = hp.model.hidden\n",
    "\n",
    "        self.rnn_x = nn.LSTM(\n",
    "            input_size=self.num_hidden,\n",
    "            hidden_size=self.num_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.rnn_y = nn.LSTM(\n",
    "            input_size=self.num_hidden,\n",
    "            hidden_size=self.num_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.W = nn.Linear(4 * self.num_hidden, self.num_hidden)\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        self.rnn_x.flatten_parameters()\n",
    "        self.rnn_y.flatten_parameters()\n",
    "\n",
    "    def forward(self, inp, audio_lengths):\n",
    "        self.flatten_parameters()\n",
    "        \n",
    "        B, M, T, D = inp.size()\n",
    "\n",
    "        inp_temp = inp.view(-1, T, D)\n",
    "        inp_temp = nn.utils.rnn.pack_padded_sequence(\n",
    "            inp_temp,\n",
    "            audio_lengths.unsqueeze(1).repeat(1, M).reshape(-1),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        x, _ = self.rnn_x(inp_temp)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            x,\n",
    "            batch_first=True,\n",
    "            total_length=T\n",
    "        )\n",
    "        x = x.view(B, M, T, 2 * D)\n",
    "\n",
    "        y, _ = self.rnn_y(inp.transpose(1, 2).contiguous().view(-1, M, D))\n",
    "        y = y.view(B, T, M, 2 * D).transpose(1, 2).contiguous()\n",
    "\n",
    "        z = torch.cat([x, y], dim=-1)\n",
    "\n",
    "        output = inp + self.W(z)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attention from \"tts.py\"\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(Attention, self).__init__()\n",
    "        self.M = hp.model.gmm\n",
    "        self.rnn_cell = nn.LSTMCell(\n",
    "            input_size=2*hp.model.hidden,\n",
    "            hidden_size=hp.model.hidden\n",
    "        )\n",
    "        self.W_g = nn.Linear(hp.model.hidden, 3*self.M)\n",
    "        \n",
    "    def attention(self, h_i, memory, ksi):\n",
    "        phi_hat = self.W_g(h_i)\n",
    "\n",
    "        ksi = ksi + torch.exp(phi_hat[:, :self.M])\n",
    "        beta = torch.exp(phi_hat[:, self.M:2*self.M])\n",
    "        alpha = F.softmax(phi_hat[:, 2*self.M:3*self.M], dim=-1)\n",
    "        \n",
    "        u = memory.new_tensor(np.arange(memory.size(1)), dtype=torch.float)\n",
    "        u_R = u + 1.5\n",
    "        u_L = u + 0.5\n",
    "        \n",
    "        term1 = torch.sum(\n",
    "            alpha.unsqueeze(-1) * torch.sigmoid(\n",
    "                (u_R - ksi.unsqueeze(-1)) / beta.unsqueeze(-1)\n",
    "            ),\n",
    "            keepdim=True,\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        term2 = torch.sum(\n",
    "            alpha.unsqueeze(-1) * torch.sigmoid(\n",
    "                (u_L - ksi.unsqueeze(-1)) / beta.unsqueeze(-1)\n",
    "            ),\n",
    "            keepdim=True,\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        weights = term1 - term2\n",
    "        \n",
    "        context = torch.bmm(weights, memory)\n",
    "        \n",
    "        termination = 1 - term1.squeeze(1)\n",
    "\n",
    "        return context, weights, termination, ksi # (B, 1, D), (B, 1, T), (B, T)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, input_h_c, memory):\n",
    "        B, T, D = input_h_c.size()\n",
    "        \n",
    "        context = input_h_c.new_zeros(B, D)\n",
    "        h_i, c_i  = input_h_c.new_zeros(B, D), input_h_c.new_zeros(B, D)\n",
    "        ksi = input_h_c.new_zeros(B, self.M)\n",
    "        \n",
    "        contexts, weights = [], []\n",
    "        for i in range(T):\n",
    "            x = torch.cat([input_h_c[:, i], context.squeeze(1)], dim=-1)\n",
    "            h_i, c_i = self.rnn_cell(x, (h_i, c_i))\n",
    "            context, weight, termination, ksi = self.attention(h_i, memory, ksi)\n",
    "            \n",
    "            contexts.append(context)\n",
    "            weights.append(weight)\n",
    "            \n",
    "        contexts = torch.cat(contexts, dim=1) + input_h_c\n",
    "        alignment = torch.cat(weights, dim=1)\n",
    "        # termination = torch.gather(termination, 1, (input_lengths-1).unsqueeze(-1)) # 4\n",
    "\n",
    "        return contexts, alignment#, termination\n",
    "\n",
    "\n",
    "\n",
    "class TTS(nn.Module):\n",
    "    def __init__(self, hp, freq, layers):\n",
    "        super(TTS, self).__init__()\n",
    "        self.hp = hp\n",
    "\n",
    "        self.W_t_0 = nn.Linear(1, hp.model.hidden)\n",
    "        self.W_f_0 = nn.Linear(1, hp.model.hidden)\n",
    "        self.W_c_0 = nn.Linear(freq, hp.model.hidden)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DelayedRNN(hp) for _ in range(layers)])\n",
    "\n",
    "        # Gaussian Mixture Model: eq. (2)\n",
    "        self.K = hp.model.gmm\n",
    "\n",
    "        # map output to produce GMM parameter eq. (10)\n",
    "        self.W_theta = nn.Linear(hp.model.hidden, 3*self.K)\n",
    "\n",
    "        if self.hp.data.name == 'KSS':\n",
    "            self.embedding_text = nn.Embedding(len(symbols), hp.model.hidden)\n",
    "        elif self.hp.data.name == 'Blizzard':\n",
    "            self.embedding_text = nn.Embedding(len(en_symbols), hp.model.hidden)\n",
    "        elif self.hp.data.name == 'Trump':\n",
    "            self.embedding_text = nn.Embedding(len(en_symbols), hp.model.hidden)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.text_lstm = nn.LSTM(\n",
    "            input_size=hp.model.hidden,\n",
    "            hidden_size=hp.model.hidden//2, \n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.attention = Attention(hp)\n",
    "\n",
    "    def text_encode(self, text, text_lengths):\n",
    "        total_length = text.size(1)\n",
    "        embed = self.embedding_text(text)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embed,\n",
    "            text_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        memory, _ = self.text_lstm(packed)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            memory,\n",
    "            batch_first=True,\n",
    "            total_length=total_length\n",
    "        )\n",
    "        return unpacked\n",
    "        \n",
    "    def forward(self, x, text, text_lengths, audio_lengths):\n",
    "        # Extract memory\n",
    "        memory = self.text_encode(text, text_lengths)\n",
    "        \n",
    "        # x: [B, M, T] / B=batch, M=mel, T=time\n",
    "        h_t = self.W_t_0(F.pad(x, [1, -1]).unsqueeze(-1))\n",
    "        h_f = self.W_f_0(F.pad(x, [0, 0, 1, -1]).unsqueeze(-1))\n",
    "        h_c = self.W_c_0(F.pad(x, [1, -1]).transpose(1, 2))\n",
    "        \n",
    "        # h_t, h_f: [B, M, T, D] / h_c: [B, T, D]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != (len(self.layers)//2):\n",
    "                h_t, h_f, h_c = layer(h_t, h_f, h_c, audio_lengths)\n",
    "                \n",
    "            else:\n",
    "                h_c, alignment = self.attention(h_c, memory)\n",
    "                h_t, h_f, h_c = layer(h_t, h_f, h_c, audio_lengths)\n",
    "\n",
    "        theta_hat = self.W_theta(h_f)\n",
    "\n",
    "        mu = theta_hat[..., :self.K] # eq. (3)\n",
    "        std = theta_hat[..., self.K:2*self.K] # eq. (4)\n",
    "        pi = theta_hat[..., 2*self.K:] # eq. (5)\n",
    "            \n",
    "        return mu, std, pi, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GMMLoss from \"loss.py\"\n",
    "class GMMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GMMLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, mu, std, pi, audio_lengths):\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x.unsqueeze(-1).transpose(1, 2),\n",
    "            audio_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ).data\n",
    "        mu = nn.utils.rnn.pack_padded_sequence(\n",
    "            mu.transpose(1, 2),\n",
    "            audio_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ).data\n",
    "        std = nn.utils.rnn.pack_padded_sequence(\n",
    "            std.transpose(1, 2),\n",
    "            audio_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ).data\n",
    "        pi = nn.utils.rnn.pack_padded_sequence(\n",
    "            pi.transpose(1, 2),\n",
    "            audio_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ).data\n",
    "        log_prob = Normal(loc=mu, scale=std.exp()).log_prob(x)\n",
    "        log_distrib = log_prob + F.log_softmax(pi, dim=-1)\n",
    "        loss = -torch.logsumexp(log_distrib, dim=-1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MelGen from \"audio.py\"\n",
    "class MelGen():\n",
    "    def __init__(self, hp):\n",
    "        self.hp = hp\n",
    "\n",
    "    def get_normalized_mel(self, x):\n",
    "        x = librosa.feature.melspectrogram(\n",
    "            y=x,\n",
    "            sr=self.hp.audio.sr,\n",
    "            n_fft=self.hp.audio.win_length,\n",
    "            hop_length=self.hp.audio.hop_length,\n",
    "            win_length=self.hp.audio.win_length,\n",
    "            n_mels=self.hp.audio.n_mels\n",
    "        )\n",
    "        x = self.pre_spec(x)\n",
    "        return x\n",
    "\n",
    "    def pre_spec(self, x):\n",
    "        return self.normalize(librosa.power_to_db(x) - self.hp.audio.ref_level_db)\n",
    "\n",
    "    def post_spec(self, x):\n",
    "        return librosa.db_to_power(self.denormalize(x) + self.hp.audio.ref_level_db)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return np.clip(x / -self.hp.audio.min_level_db, -1.0, 0.0) + 1.0\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        return (np.clip(x, 0.0, 1.0) - 1.0) * -self.hp.audio.min_level_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TierUtil from \"tierutil.py\"\n",
    "class TierUtil():\n",
    "    def __init__(self, hp):\n",
    "        self.hp = hp\n",
    "        self.n_mels = hp.audio.n_mels\n",
    "\n",
    "        self.f_div = f_div[hp.model.tier]\n",
    "        self.t_div = t_div[hp.model.tier]\n",
    "\n",
    "        # when we perform stft, the number of time frames we get is:\n",
    "        # self.T = int(hp.audio.sr * hp.audio.duration) // hp.audio.hop_length + 1\n",
    "        # 10*22050 // 256 + 1 = 862 (blizzard)\n",
    "        # 6*22050 // 256 + 1 = 517 (maestro)\n",
    "        # 6*16000 // 180 + 1 = 534 (voxceleb2)\n",
    "        # 10*16000 // 180 + 1 = 889 (tedlium3)        \n",
    "\n",
    "    def cut_divide_tiers(self, x, tierNo):\n",
    "        x = x[:, :x.shape[-1] - x.shape[-1] % self.t_div]\n",
    "        M, T = x.shape\n",
    "        assert M % self.f_div == 0, \\\n",
    "            'freq(mel) dimension should be divisible by %d, got %d.' \\\n",
    "            % (self.f_div, M)\n",
    "        assert T % self.t_div == 0, \\\n",
    "            'time dimension should be divisible by %d, got %d.' \\\n",
    "            % (self.t_div, T)\n",
    "\n",
    "        tiers = list()\n",
    "        for i in range(self.hp.model.tier, max(1, tierNo-1), -1):\n",
    "            if i % 2 == 0: # make consistent with utils/constant.py\n",
    "                tiers.append(x[1::2, :])\n",
    "                x = x[::2, :]\n",
    "            else:\n",
    "                tiers.append(x[:, 1::2])\n",
    "                x = x[:, ::2]\n",
    "        tiers.append(x)\n",
    "\n",
    "        # return source, target\n",
    "        if tierNo == 1:\n",
    "            return tiers[-1], tiers[-1].copy()\n",
    "        else:\n",
    "            return tiers[-1], tiers[-2]\n",
    "\n",
    "    def interleave(self, x, y, tier):\n",
    "        '''\n",
    "            implements eq. (25)\n",
    "            x: x^{<g}\n",
    "            y: x^{g}\n",
    "            tier: g+1\n",
    "        '''\n",
    "        assert x.size() == y.size(), \\\n",
    "            'two inputs for interleave should be identical: got %s, %s' % (x.size(), y.size())\n",
    "\n",
    "        B, M, T = x.size()\n",
    "        if tier % 2 == 0:\n",
    "            temp = x.new_zeros(B, M, 2 * T)\n",
    "            temp[:, :, 0::2] = x\n",
    "            temp[:, :, 1::2] = y\n",
    "        else:\n",
    "            temp = x.new_zeros(B, 2 * M, T)\n",
    "            temp[:, 0::2, :] = x\n",
    "            temp[:, 1::2, :] = y\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main function of \"trainer.py\"\n",
    "if __name__ == '__main__':\n",
    "    convert_to_en_symbols()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c', '--config', type=str, required=True,\n",
    "                        help=\"yaml file for configuration\")\n",
    "    parser.add_argument('-p', '--checkpoint_path', type=str, default=None,\n",
    "                        help=\"path of checkpoint pt file to resume training\")\n",
    "    parser.add_argument('-n', '--name', type=str, required=True,\n",
    "                        help=\"name of the model for logging, saving checkpoint\")\n",
    "    parser.add_argument('-t', '--tier', type=int, required=True,\n",
    "                        help=\"Number of tier to train\")\n",
    "    parser.add_argument('-b', '--batch_size', type=int, required=True,\n",
    "                        help=\"Batch size\")\n",
    "    parser.add_argument('-s', '--tts', type=bool, default=False, required=False,\n",
    "                        help=\"TTS\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    hp = HParam(args.config)\n",
    "    with open(args.config, 'r') as f:\n",
    "        hp_str = ''.join(f.readlines())\n",
    "    if platform.system() == 'Windows':\n",
    "        hp.train.num_workers = 0\n",
    "\n",
    "    pt_dir = os.path.join(hp.log.chkpt_dir, args.name)\n",
    "    log_dir = os.path.join(hp.log.log_dir, args.name)\n",
    "    if not os.path.isdir(hp.log.log_dir):\n",
    "        os.mkdir(hp.log.log_dir)\n",
    "    if not os.path.isdir(hp.log.chkpt_dir):\n",
    "        os.mkdir(hp.log.chkpt_dir)\n",
    "    if not os.path.isdir(pt_dir):\n",
    "        os.mkdir(pt_dir)\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(os.path.join(log_dir,\n",
    "                '%s-%d.log' % (args.name, time.time()))),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    writer = MyWriter(hp, log_dir)\n",
    "\n",
    "    assert hp.data.path != '', \\\n",
    "        'hp.data.path cannot be empty: please fill out your dataset\\'s path in configuration yaml file.'\n",
    "    trainloader = create_dataloader(hp, args, train=True)\n",
    "    testloader = create_dataloader(hp, args, train=False)\n",
    "\n",
    "    train(args, pt_dir, args.checkpoint_path, trainloader, testloader, writer, logger, hp, hp_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (MelNet)",
   "language": "python",
   "name": "pycharm-72c7a56d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}